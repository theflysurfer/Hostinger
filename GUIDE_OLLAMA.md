# ü§ñ Guide Ollama - API ML/AI sur VPS

> **Guide complet pour Ollama sur VPS Hostinger avec Nginx reverse proxy et SSL**

---

## üìã Vue d'ensemble

**Ollama** : Framework pour ex√©cuter des LLMs (Large Language Models) localement
**Utilisation** : API REST pour g√©n√©rer du texte, chat, embeddings
**Avantage** : Mod√®les locaux, pas de d√©pendance cloud, gratuit
**D√©ploiement** : Service systemd + Nginx reverse proxy avec HTTPS

---

## üèóÔ∏è Architecture

```
Client (HTTPS)
    ‚Üì
Nginx (443) - ollama.srv759970.hstgr.cloud
    ‚Üì reverse proxy
Ollama Service (localhost:11434)
    ‚Üì
Mod√®les LLMs locaux
```

**Configuration actuelle** :
- **Service** : `ollama.service` (systemd)
- **Port local** : 11434 (127.0.0.1 uniquement)
- **URL publique** : https://ollama.srv759970.hstgr.cloud
- **SSL** : Let's Encrypt
- **Mod√®les install√©s** : 4 (qwen2.5:14b, qwen2.5vl:7b, mistral:7b, llama3.2:1b)

---

## ‚úÖ √âtat actuel du service

### V√©rifier le service

```bash
# Statut du service
ssh root@69.62.108.82 "systemctl status ollama"

# V√©rifier si Ollama √©coute
ssh root@69.62.108.82 "netstat -tulpn | grep 11434"
# R√©sultat attendu : tcp 0 0 127.0.0.1:11434 0.0.0.0:* LISTEN

# Tester localement
ssh root@69.62.108.82 "curl -s http://localhost:11434/api/tags | jq '.models | length'"
# R√©sultat attendu : nombre de mod√®les (ex: 4)
```

### Mod√®les disponibles

```bash
# Lister les mod√®les install√©s
ssh root@69.62.108.82 "curl -s http://localhost:11434/api/tags | jq -r '.models[] | .name'"
```

**Mod√®les actuels** :
- `qwen2.5:14b` - Mod√®le chinois multilingue (14B param√®tres)
- `qwen2.5vl:7b` - Vision + Language (7B param√®tres)
- `mistral:7b` - Mod√®le fran√ßais/anglais performant
- `llama3.2:1b` - Mod√®le l√©ger et rapide (1B param√®tres)

---

## üîß Configuration Nginx avec SSL

### Configuration actuelle

**Fichier** : `/etc/nginx/sites-available/ollama-https`

```nginx
# HTTP -> HTTPS redirect
server {
    listen 80;
    listen [::]:80;
    server_name ollama.srv759970.hstgr.cloud;
    return 301 https://$host$request_uri;
}

# HTTPS
server {
    listen 443 ssl http2;
    listen [::]:443 ssl http2;
    server_name ollama.srv759970.hstgr.cloud;

    ssl_certificate /etc/letsencrypt/live/ollama.srv759970.hstgr.cloud/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/ollama.srv759970.hstgr.cloud/privkey.pem;

    access_log /var/log/nginx/ollama-access.log;
    error_log /var/log/nginx/ollama-error.log;

    location / {
        proxy_pass http://127.0.0.1:11434;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # Timeouts augment√©s pour g√©n√©ration de texte
        proxy_read_timeout 600s;
        proxy_connect_timeout 75s;

        # D√©sactiver buffering pour streaming
        proxy_buffering off;
        proxy_cache off;

        # CORS headers
        add_header Access-Control-Allow-Origin * always;
        add_header Access-Control-Allow-Methods "GET, POST, OPTIONS, DELETE, PUT" always;
        add_header Access-Control-Allow-Headers "Content-Type, Authorization" always;
    }

    # Security headers
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-XSS-Protection "1; mode=block" always;
}
```

### Points cl√©s de la configuration

1. **Timeouts augment√©s** : `proxy_read_timeout 600s` pour permettre la g√©n√©ration de longues r√©ponses
2. **Buffering d√©sactiv√©** : `proxy_buffering off` pour le streaming de tokens
3. **CORS activ√©** : Permet les appels depuis n'importe quel domaine
4. **HTTP/2** : `http2` pour meilleures performances
5. **Security headers** : Protection XSS, clickjacking, etc.

---

## üöÄ D√©ploiement initial (si besoin)

### √âtape 1 : Installer Ollama

```bash
# Installation
ssh root@69.62.108.82 "curl -fsSL https://ollama.com/install.sh | sh"

# V√©rifier l'installation
ssh root@69.62.108.82 "ollama --version"
```

### √âtape 2 : Configurer le service systemd

```bash
# V√©rifier la config du service
ssh root@69.62.108.82 "systemctl cat ollama.service"
```

**Config standard** : `/etc/systemd/system/ollama.service`

```ini
[Unit]
Description=Ollama Service
After=network.target

[Service]
ExecStart=/usr/local/bin/ollama serve
User=root
Restart=always
RestartSec=3

[Install]
WantedBy=multi-user.target
```

### √âtape 3 : D√©marrer le service

```bash
# D√©marrer et activer au boot
ssh root@69.62.108.82 "systemctl enable --now ollama"

# V√©rifier
ssh root@69.62.108.82 "systemctl status ollama"
```

### √âtape 4 : Installer des mod√®les

```bash
# Installer un mod√®le (exemple : llama3.2:1b)
ssh root@69.62.108.82 "ollama pull llama3.2:1b"

# Lister les mod√®les
ssh root@69.62.108.82 "ollama list"
```

**Mod√®les recommand√©s** :
- `llama3.2:1b` - L√©ger et rapide (1.3GB)
- `mistral:7b` - Bon compromis qualit√©/taille (4.1GB)
- `qwen2.5:14b` - Tr√®s performant mais lourd (9GB)

### √âtape 5 : Configurer SSL avec Certbot

```bash
# 1. Arr√™ter Nginx
ssh root@69.62.108.82 "systemctl stop nginx"

# 2. Obtenir le certificat SSL
ssh root@69.62.108.82 "certbot certonly --standalone -d ollama.srv759970.hstgr.cloud --non-interactive --agree-tos --email contact@srv759970.hstgr.cloud"

# 3. Cr√©er la config Nginx (voir section ci-dessus)
# 4. Activer et red√©marrer
ssh root@69.62.108.82 "ln -s /etc/nginx/sites-available/ollama-https /etc/nginx/sites-enabled/"
ssh root@69.62.108.82 "nginx -t && systemctl start nginx"
```

---

## üß™ Tester l'API

### Endpoints disponibles

#### 1. Lister les mod√®les

```bash
curl https://ollama.srv759970.hstgr.cloud/api/tags
```

**R√©ponse** :
```json
{
  "models": [
    {
      "name": "llama3.2:1b",
      "modified_at": "2025-10-10T11:28:35Z",
      "size": 1300000000
    }
  ]
}
```

#### 2. G√©n√©rer du texte

```bash
curl -X POST https://ollama.srv759970.hstgr.cloud/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.2:1b",
    "prompt": "Pourquoi le ciel est bleu?",
    "stream": false
  }'
```

**R√©ponse** :
```json
{
  "model": "llama3.2:1b",
  "response": "Le ciel appara√Æt bleu car...",
  "done": true
}
```

#### 3. Chat (conversation)

```bash
curl -X POST https://ollama.srv759970.hstgr.cloud/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistral:7b",
    "messages": [
      {"role": "user", "content": "Bonjour, qui es-tu?"}
    ],
    "stream": false
  }'
```

#### 4. Embeddings (vecteurs)

```bash
curl -X POST https://ollama.srv759970.hstgr.cloud/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.2:1b",
    "prompt": "Hello world"
  }'
```

### Mode streaming

Pour recevoir les tokens au fur et √† mesure :

```bash
curl -X POST https://ollama.srv759970.hstgr.cloud/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.2:1b",
    "prompt": "√âcris une histoire courte",
    "stream": true
  }'
```

**R√©sultat** : Flux de JSON (un par ligne)

---

## üîß Gestion des mod√®les

### Installer un nouveau mod√®le

```bash
# Depuis le VPS
ssh root@69.62.108.82 "ollama pull [model-name]"

# Exemples
ssh root@69.62.108.82 "ollama pull llama3.2:1b"
ssh root@69.62.108.82 "ollama pull mistral:7b"
ssh root@69.62.108.82 "ollama pull qwen2.5:14b"
```

### Supprimer un mod√®le

```bash
ssh root@69.62.108.82 "ollama rm [model-name]"

# Exemple
ssh root@69.62.108.82 "ollama rm llama3.2:1b"
```

### Voir l'espace disque utilis√©

```bash
# Taille totale des mod√®les
ssh root@69.62.108.82 "du -sh ~/.ollama/models/"

# D√©tail par mod√®le
ssh root@69.62.108.82 "ollama list"
```

---

## üêõ Troubleshooting

### Probl√®me 1 : Service ne d√©marre pas

```bash
# Voir les logs
ssh root@69.62.108.82 "journalctl -u ollama -n 50 --no-pager"

# Red√©marrer le service
ssh root@69.62.108.82 "systemctl restart ollama"
```

### Probl√®me 2 : Nginx retourne 502 Bad Gateway

**Cause** : Ollama n'est pas d√©marr√© ou n'√©coute pas sur 11434

**Solution** :
```bash
# V√©rifier le service
ssh root@69.62.108.82 "systemctl status ollama"

# V√©rifier le port
ssh root@69.62.108.82 "netstat -tulpn | grep 11434"

# Red√©marrer si n√©cessaire
ssh root@69.62.108.82 "systemctl restart ollama"
```

### Probl√®me 3 : Timeout sur g√©n√©ration

**Cause** : Timeout Nginx trop court

**Solution** : Augmenter `proxy_read_timeout` dans la config Nginx

```nginx
proxy_read_timeout 600s;  # 10 minutes
```

### Probl√®me 4 : Mod√®le ne charge pas (out of memory)

**Sympt√¥mes** : Erreur "failed to load model"

**Solution** : Utiliser un mod√®le plus petit ou augmenter la RAM du VPS

```bash
# V√©rifier la RAM disponible
ssh root@69.62.108.82 "free -h"

# Utiliser un mod√®le plus l√©ger
ssh root@69.62.108.82 "ollama pull llama3.2:1b"  # Au lieu de qwen2.5:14b
```

---

## üìä Monitoring et logs

### Logs du service

```bash
# Logs en temps r√©el
ssh root@69.62.108.82 "journalctl -u ollama -f"

# Derni√®res 100 lignes
ssh root@69.62.108.82 "journalctl -u ollama -n 100 --no-pager"

# Logs depuis une date
ssh root@69.62.108.82 "journalctl -u ollama --since '2025-10-15 10:00:00'"
```

### Logs Nginx

```bash
# Access logs
ssh root@69.62.108.82 "tail -f /var/log/nginx/ollama-access.log"

# Error logs
ssh root@69.62.108.82 "tail -f /var/log/nginx/ollama-error.log"
```

### Statistiques d'utilisation

```bash
# Voir l'utilisation CPU/RAM
ssh root@69.62.108.82 "top -b -n 1 | grep ollama"

# Voir les processus Ollama
ssh root@69.62.108.82 "ps aux | grep ollama"
```

---

## üîê S√©curit√©

### Consid√©rations importantes

1. **Pas d'authentification native** : Ollama n'a pas d'auth built-in
2. **CORS ouvert** : Config actuelle permet tous les origins
3. **Rate limiting** : Pas de limite de requ√™tes

### Ajouter une authentification (optionnel)

#### Option 1 : Basic Auth Nginx

```nginx
location / {
    auth_basic "Ollama API";
    auth_basic_user_file /etc/nginx/.htpasswd;

    proxy_pass http://127.0.0.1:11434;
    # ... reste de la config
}
```

```bash
# Cr√©er le fichier de mots de passe
ssh root@69.62.108.82 "apt install -y apache2-utils"
ssh root@69.62.108.82 "htpasswd -c /etc/nginx/.htpasswd admin"
```

#### Option 2 : API Key via header

```nginx
location / {
    if ($http_x_api_key != "votre-secret-key") {
        return 403;
    }

    proxy_pass http://127.0.0.1:11434;
    # ... reste de la config
}
```

**Utilisation** :
```bash
curl -H "X-API-Key: votre-secret-key" https://ollama.srv759970.hstgr.cloud/api/tags
```

---

## üìö Ressources

- **Ollama Documentation** : https://github.com/ollama/ollama
- **API Reference** : https://github.com/ollama/ollama/blob/main/docs/api.md
- **Mod√®les disponibles** : https://ollama.com/library
- **Guide Nginx** : [GUIDE_NGINX.md](./GUIDE_NGINX.md)

---

## üìù Checklist d√©ploiement Ollama

- [ ] Ollama install√© (`curl -fsSL https://ollama.com/install.sh | sh`)
- [ ] Service systemd actif (`systemctl enable --now ollama`)
- [ ] Au moins un mod√®le install√© (`ollama pull llama3.2:1b`)
- [ ] Service √©coute sur 11434 (`netstat -tulpn | grep 11434`)
- [ ] Certificat SSL obtenu (Certbot standalone)
- [ ] Config Nginx cr√©√©e avec timeouts augment√©s
- [ ] Config Nginx test√©e (`nginx -t`)
- [ ] Site activ√© (symlink dans `sites-enabled/`)
- [ ] Nginx recharg√© (`systemctl reload nginx`)
- [ ] API accessible via HTTPS
- [ ] Test g√©n√©ration de texte r√©ussi

---

**Derni√®re mise √† jour** : Octobre 2025
**Version** : 1.0
**Status** : ‚úÖ Op√©rationnel sur srv759970.hstgr.cloud
**URL publique** : https://ollama.srv759970.hstgr.cloud
**Mod√®les install√©s** : 4 (qwen2.5:14b, qwen2.5vl:7b, mistral:7b, llama3.2:1b)
